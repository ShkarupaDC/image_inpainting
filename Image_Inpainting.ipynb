{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Inpainting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4cl6IBHcUtUVhZBZjfuq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShkarupaDC/image_inpainting/blob/master/Image_Inpainting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQk1-YWu7M5s"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from collections import namedtuple"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tt2Yuw2-AAn"
      },
      "source": [
        "class PConv2d(nn.Conv2d):\n",
        "\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    \n",
        "    if \"return_mask\" in kwargs:\n",
        "      \n",
        "      self.return_mask = kwargs[\"return_mask\"]\n",
        "      kwargs.pop(\"return_mask\")\n",
        "    \n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.window = self.in_channels * self.out_channels *\\\n",
        "          self.kernel_size[0] * self.kernel_size[0]\n",
        "    \n",
        "    self.mask_kernel = torch.ones(self.out_channels,\n",
        "          self.in_channels, *self.kernel_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if mask is not None:\n",
        "\n",
        "        updated_mask = F.conv2d(mask, self.mask_kernel, None,\n",
        "            self.stride, self.padding, self.dilation)\n",
        "        \n",
        "        mask_ratio = self.window / (updated_mask + 1e-6)\n",
        "        \n",
        "        updated_mask = torch.clamp(updated_mask, 0, 1)\n",
        "        mask_ratio = torch.mul(updated_mask, mask_ratio)\n",
        "    \n",
        "    x = super().forward(x if mask is None else torch.mul(x, mask))\n",
        "\n",
        "    if self.bias is not None:\n",
        "      \n",
        "      bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "      x = torch.mul(x, mask_ratio) + bias_view\n",
        "\n",
        "    else: x = torch.mul(x, mask_ratio)\n",
        "\n",
        "    if self.return_mask is True: return (x, updated_mask)\n",
        "    \n",
        "    else: return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVYCG6dcFIpW"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size,\n",
        "      stride=1, padding=0, bn=True, activation=nn.ReLU()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pconv = PConv2d(in_channels, out_channels,\n",
        "        kernel_size, stride, padding, return_mask=True)\n",
        "    \n",
        "    if bn is True:\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.activation = activation\n",
        "\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    \n",
        "    x, mask = self.pconv(x, mask)\n",
        "\n",
        "    if hasattr(self, \"bn\"):\n",
        "      x = self.bn(x)\n",
        "\n",
        "    x = self.activation(x)\n",
        "    \n",
        "    return x, mask"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLnCftLTFaSF"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size,\n",
        "    stride=1, padding=0, bn=True, activation=nn.LeakyReLU(0.2)):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pconv = PConv2d(in_channels + out_channels, out_channels,\n",
        "      kernel_size, stride, padding, return_mask=True)\n",
        "    \n",
        "    if bn is True:\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    self.activation = activation\n",
        "\n",
        "\n",
        "  def forward(self, x, mask, encoded_x, encoded_mask):\n",
        "\n",
        "    x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "    mask = F.interpolate(mask, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "    x = torch.cat([x, encoded_x], dim=1)\n",
        "    mask = torch.cat([mask, encoded_mask], dim=1)\n",
        "\n",
        "    x, mask = self.pconv(x, mask)\n",
        "\n",
        "    if hasattr(self, \"bn\"): x = self.bn(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    return x, mask\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cmoyEhXtLSH"
      },
      "source": [
        "UNetBlock = namedtuple(\"UNetBlock\", [\"in_channels\", \"out_channels\",\n",
        "    \"kernel_size\", \"stride\", \"padding\", \"bn\"])\n",
        "\n",
        "EncoderBlock = [\n",
        "  UNetBlock(3, 64, 7, 2, 3, False), UNetBlock(64, 128, 5, 2, 2, True),\n",
        "  UNetBlock(128, 256, 3, 2, 1, True), UNetBlock(256, 512, 3, 2, 1, True),\n",
        "  UNetBlock(512, 512, 3, 2, 1, True), UNetBlock(512, 512, 3, 2, 1, True),\n",
        "  UNetBlock(512, 512, 3, 2, 1, True), UNetBlock(512, 512, 3, 2, 1, True)\n",
        "]\n",
        "\n",
        "DecoderBlock = [\n",
        "  UNetBlock(512, 512, 3, 1, 1, True), UNetBlock(512, 512, 3, 1, 1, True),\n",
        "  UNetBlock(512, 512, 3, 1, 1, True), UNetBlock(512, 512, 3, 1, 1, True),\n",
        "  UNetBlock(512, 256, 3, 1, 1, True), UNetBlock(256, 128, 3, 1, 1, True),\n",
        "  UNetBlock(128, 64, 3, 1, 1, True), UNetBlock(64, 3, 3, 1, 1, False)\n",
        "]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE8jiyQQOJjY"
      },
      "source": [
        "class UNetModule(nn.Module):\n",
        "\n",
        "  def __init__(self, e_blocks, d_blocks):\n",
        "    super().__init__()\n",
        "\n",
        "    self.e_layers, self.d_layers = [], []\n",
        "    blocks = zip(e_blocks, d_blocks)\n",
        "\n",
        "    for idx, (e_block, d_block) in enumerate(blocks):\n",
        "      \n",
        "      self.e_layers.append(EncoderLayer(*e_block))\n",
        "      self.d_layers.append(DecoderLayer(*d_block))\n",
        "\n",
        "    self.e_layers = nn.ModuleList(self.e_layers)\n",
        "    self.d_layers = nn.ModuleList(self.d_layers)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \n",
        "    encoded = [(x, mask)]\n",
        "\n",
        "    for e_layer in self.e_layers:\n",
        "      x, mask = e_layer(x, mask)\n",
        "      encoded.append((x, mask))\n",
        "\n",
        "    for idx, d_layer in enumerate(self.d_layers):\n",
        "      x, mask = d_layer(x, mask, *encoded[-(idx + 2)])\n",
        "\n",
        "    return x, mask\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18HxNPx2zKcN"
      },
      "source": [
        "unet = UNetModule(EncoderBlock, DecoderBlock)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ORsMxv34JwA",
        "outputId": "02a8319a-648b-427b-d5bb-84a72594f6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unet(torch.Tensor(1, 3, 512, 512), torch.ones((1, 3, 512, 512)))"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[[ 8.6368e-01,  2.1252e+00,  1.3758e+00,  ...,  1.7904e+00,\n",
              "             2.1906e+00,  2.5858e+00],\n",
              "           [ 1.0734e-02, -3.9869e-03, -1.2115e-01,  ..., -7.0984e-02,\n",
              "            -6.9399e-02,  2.7649e-02],\n",
              "           [-9.7896e-02, -3.8180e-02, -1.5243e-01,  ..., -4.9820e-02,\n",
              "            -4.3078e-02,  2.3399e-01],\n",
              "           ...,\n",
              "           [-6.1077e+00, -9.7851e+00, -1.7425e+01,  ..., -9.2996e+00,\n",
              "            -8.5264e+00, -9.2138e+00],\n",
              "           [-4.9272e+00, -1.4002e+01, -1.6254e+01,  ..., -1.0073e+01,\n",
              "            -6.9511e+00, -1.6341e+01],\n",
              "           [-1.0363e+01, -2.3281e+01, -2.2901e+01,  ..., -1.3245e+01,\n",
              "            -1.6378e+01, -1.2668e+01]],\n",
              " \n",
              "          [[-4.9273e-01, -2.3594e-01, -2.0951e-01,  ..., -2.3786e-01,\n",
              "            -1.8807e-01,  9.2741e-01],\n",
              "           [-5.8254e-01, -1.7978e-01, -1.7195e-01,  ..., -1.5349e-01,\n",
              "            -1.2689e-01,  2.7418e-01],\n",
              "           [-4.6452e-01, -2.2543e-01, -1.7828e-01,  ..., -1.0264e-01,\n",
              "            -7.2852e-02,  5.1800e-01],\n",
              "           ...,\n",
              "           [ 8.1142e+01,  2.8314e+01,  2.8278e+01,  ...,  2.1734e+01,\n",
              "             4.1498e+01,  4.2462e+01],\n",
              "           [ 1.2813e+01, -5.6451e+00, -1.3330e+00,  ..., -1.0333e+00,\n",
              "             2.3696e-01, -2.1726e+00],\n",
              "           [ 3.4820e+01,  6.8446e+01,  5.2224e+01,  ...,  3.3352e+01,\n",
              "             2.6625e+01,  9.6301e+01]],\n",
              " \n",
              "          [[ 2.7871e+00,  2.0203e+00,  2.6144e+00,  ...,  3.0735e+00,\n",
              "             2.8162e+00,  4.1921e+00],\n",
              "           [-3.0859e-02,  2.4275e-01,  6.5557e-01,  ...,  1.1285e+00,\n",
              "             1.0399e+00,  2.3075e+00],\n",
              "           [ 2.4542e-01,  6.7475e-01,  1.1888e+00,  ...,  1.0320e+00,\n",
              "             9.1704e-01,  2.1646e+00],\n",
              "           ...,\n",
              "           [ 8.8314e+01,  4.6610e+01,  8.4927e+01,  ...,  4.5863e+01,\n",
              "             5.1173e+01,  5.0850e+01],\n",
              "           [ 1.2003e+02,  5.8981e+01,  1.0039e+02,  ...,  5.6700e+01,\n",
              "             4.8494e+01,  5.6086e+01],\n",
              "           [-1.7082e+01,  8.0422e+01,  6.4113e+01,  ...,  4.6171e+01,\n",
              "             4.7457e+01,  1.1909e+02]]]], grad_fn=<LeakyReluBackward0>),\n",
              " tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           ...,\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "           [1., 1., 1.,  ..., 1., 1., 1.]]]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgvf6Fkd5OSJ"
      },
      "source": [
        ""
      ],
      "execution_count": 138,
      "outputs": []
    }
  ]
}